#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import logging
import sys
import argparse
import time
from urllib.parse import urlparse

# --- Renkli log formatÄ± ---
class RenkliFormatter(logging.Formatter):
    renkler = {
        logging.DEBUG: "\033[90m",   # Gri
        logging.INFO: "\033[92m",    # YeÅŸil
        logging.WARNING: "\033[93m", # SarÄ±
        logging.ERROR: "\033[91m",   # KÄ±rmÄ±zÄ±
        logging.CRITICAL: "\033[95m" # Mor
    }
    son = "\033[0m"

    def format(self, record):
        renk = self.renkler.get(record.levelno, self.son)
        tarih = f"[{self.formatTime(record, '%H:%M:%S')}]"
        return f"{renk}{tarih} {record.levelname:<8} {record.getMessage()}{self.son}"

# --- ArgÃ¼manlar ---
parser = argparse.ArgumentParser(description="DiziLife scraper - M3U playlist oluÅŸturucu")
parser.add_argument("--verbose", action="store_true", help="DetaylÄ± log Ã§Ä±ktÄ±sÄ±")
parser.add_argument("--output", default="playlist.m3u", help="Ã‡Ä±ktÄ± dosyasÄ± adÄ±")
args = parser.parse_args()

# --- Logging ayarÄ± ---
log_seviyesi = logging.DEBUG if args.verbose else logging.INFO
handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(RenkliFormatter())
logging.basicConfig(level=log_seviyesi, handlers=[handler])
log = logging.getLogger(__name__)

# --- Otomatik aktif domain tespiti ---
def aktif_domain_bul():
    """dizi20.life, dizi21.life, dizi22.life gibi domainleri test ederek Ã§alÄ±ÅŸanÄ± bulur"""
    for i in range(20, 30):
        domain = f"https://dizi{i}.life"
        try:
            r = requests.get(domain, timeout=5, allow_redirects=True)
            if r.status_code == 200:
                aktif = urlparse(r.url).scheme + "://" + urlparse(r.url).netloc
                log.info(f"ðŸŒ Aktif domain bulundu: {aktif}")
                return aktif
        except requests.RequestException:
            log.debug(f"{domain} aktif deÄŸil.")
    log.error("HiÃ§bir aktif domain bulunamadÄ±!")
    sys.exit(1)

BASE_URL = aktif_domain_bul()
session = requests.Session()

# --- YardÄ±mcÄ± fonksiyonlar ---
def get_page(url):
    """SayfayÄ± indirir ve yÃ¶nlendirmeleri takip eder."""
    try:
        log.debug(f"GET {url}")
        response = session.get(url, timeout=10, allow_redirects=True)
        response.raise_for_status()
        if response.url != url:
            log.debug(f"YÃ¶nlendirildi: {response.url}")
        return response.text
    except requests.RequestException as e:
        log.error(f"BaÄŸlantÄ± hatasÄ±: {url} ({e})")
        return None

def parse_links(html):
    """Sayfadaki dizi baÄŸlantÄ±larÄ±nÄ± Ã§eker."""
    soup = BeautifulSoup(html, "html.parser")
    links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "/dizi/" in href or "/film/" in href:
            links.append(href)
    return sorted(set(links))

# --- Ana iÅŸlem ---
def main():
    log.info("Manuel URL saÄŸlanmadÄ±, kategoriler otomatik taranÄ±yor...")

    kategori_url = [f"{BASE_URL}/diziler", f"{BASE_URL}/filmler"]
    tum_linkler = []

    for kategori in kategori_url:
        log.debug(f"Kategori taranÄ±yor: {kategori}")
        html = get_page(kategori)
        if html:
            links = parse_links(html)
            log.info(f"{kategori} iÃ§inde {len(links)} iÃ§erik bulundu.")
            tum_linkler.extend(links)

    tum_linkler = sorted(set(tum_linkler))
    log.info(f"Toplam {len(tum_linkler)} iÃ§erik sayfasÄ± bulundu.")

    # Playlist dosyasÄ±nÄ± oluÅŸtur
    with open(args.output, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        for link in tum_linkler:
            f.write(f"#EXTINF:-1,{link.split('/')[-1]}\n")
            full_url = link if link.startswith("http") else f"{BASE_URL}{link}"
            f.write(f"{full_url}\n")
            time.sleep(0.05)

    log.info(f"âœ… Playlist oluÅŸturuldu: {args.output}")

# --- GiriÅŸ noktasÄ± ---
if __name__ == "__main__":
    main()
