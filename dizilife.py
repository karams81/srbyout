
---

# `dizilife.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
dizilife.py
Basit, gÃ¼venli bir scraper Ã¶rneÄŸi:
- Otomatik domain tespiti (dizi20..dizi40)
- --local ile yerel HTML dosyasÄ±ndan Ã§alÄ±ÅŸma
- Esnek link parse etme
- .m3u playlist oluÅŸturma
"""

import argparse
import logging
import sys
import time
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from typing import List, Optional

# ---------------------
# Logging (renkli)
# ---------------------
class RenkliFormatter(logging.Formatter):
    renkler = {
        logging.DEBUG: "\033[90m",   # Gri
        logging.INFO: "\033[92m",    # YeÅŸil
        logging.WARNING: "\033[93m", # SarÄ±
        logging.ERROR: "\033[91m",   # KÄ±rmÄ±zÄ±
        logging.CRITICAL: "\033[95m" # Mor
    }
    son = "\033[0m"

    def format(self, record):
        renk = self.renkler.get(record.levelno, self.son)
        tarih = f"[{self.formatTime(record, '%H:%M:%S')}]"
        mesaj = super().format(record)
        # only include the message part to keep format predictable
        return f"{renk}{tarih} {record.levelname:<8} {record.getMessage()}{self.son}"

# ---------------------
# Argparse
# ---------------------
parser = argparse.ArgumentParser(description="DiziLife scraper - M3U playlist oluÅŸturucu (yerel test destekli)")
parser.add_argument("--verbose", action="store_true", help="DetaylÄ± log Ã§Ä±ktÄ±sÄ±")
parser.add_argument("--output", default="playlist.m3u", help="Ã‡Ä±ktÄ± dosyasÄ± adÄ±")
parser.add_argument("--local", help="Yerel HTML dosyasÄ± (tarayÄ±cÄ±da kaydedilmiÅŸ) ile Ã§alÄ±ÅŸ")
parser.add_argument("--domain-start", type=int, default=20, help="Domain test aralÄ±ÄŸÄ± baÅŸlangÄ±cÄ± (vars:20)")
parser.add_argument("--domain-end", type=int, default=40, help="Domain test aralÄ±ÄŸÄ± bitiÅŸi (vars:40)")
args = parser.parse_args()

log_level = logging.DEBUG if args.verbose else logging.INFO
handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(RenkliFormatter())
logging.basicConfig(level=log_level, handlers=[handler])
log = logging.getLogger(__name__)

# ---------------------
# Session & headers
# ---------------------
session = requests.Session()
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0 Safari/537.36"
}

# ---------------------
# Domain tespiti
# ---------------------
def aktif_domain_bul(start: int = 20, end: int = 40, timeout: int = 5) -> Optional[str]:
    """
    dizi{start}.life ... dizi{end}.life aralÄ±ÄŸÄ±nÄ± test eder.
    Ä°lk 200 OK dÃ¶nen domain'in scheme+netloc kÄ±smÄ±nÄ± dÃ¶ner.
    """
    log.debug(f"Domain aralÄ±ÄŸÄ± aranÄ±yor: dizi{start}.life .. dizi{end - 1}.life")
    for i in range(start, end):
        domain = f"https://dizi{i}.life"
        try:
            r = session.get(domain, timeout=timeout, allow_redirects=True, headers=DEFAULT_HEADERS)
            log.debug(f"GET {domain} -> {r.status_code} ({r.url})")
            if r.status_code == 200:
                parsed = urlparse(r.url)
                aktif = f"{parsed.scheme}://{parsed.netloc}"
                return aktif
        except requests.RequestException as e:
            log.debug(f"{domain} eriÅŸilemedi: {e}")
    return None

# ---------------------
# HTML alma (web veya local)
# ---------------------
def get_page(url: str) -> Optional[str]:
    try:
        log.debug(f"GET {url}")
        r = session.get(url, timeout=10, allow_redirects=True, headers=DEFAULT_HEADERS)
        r.raise_for_status()
        if r.url != url:
            log.debug(f"YÃ¶nlendirildi: {r.url}")
        return r.text
    except requests.RequestException as e:
        log.error(f"BaÄŸlantÄ± hatasÄ±: {url} ({e})")
        return None

def get_local_file(path: str) -> Optional[str]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        log.error(f"Yerel dosya okunamadÄ±: {path} ({e})")
        return None

# ---------------------
# Link parse etme
# ---------------------
def parse_links(html: str, base_url: str) -> List[str]:
    """
    HTML iÃ§inden dizi/film baÄŸlantÄ±larÄ±nÄ± esnekÃ§e yakalar.
    base_url ile tam URL'ler oluÅŸturulur.
    """
    soup = BeautifulSoup(html, "html.parser")
    links = []
    keywords = ["dizi", "film", "series", "konusanlar", "episode", "bolum"]
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        # Basit filtre: anahtar kelimelerden herhangi biri geÃ§iyorsa deÄŸerlendir
        if any(k in href.lower() for k in keywords):
            # Normalize et
            if href.startswith("//"):
                href = "https:" + href
            elif href.startswith("/"):
                href = base_url.rstrip("/") + href
            elif not href.startswith("http"):
                href = base_url.rstrip("/") + "/" + href.lstrip("/")
            links.append(href)
    # unique ve sÄ±ralÄ±
    return sorted(set(links))

# ---------------------
# Main
# ---------------------
def main():
    # EÄŸer local modu verilmiÅŸse onu tercih et
    if args.local:
        log.info("ğŸ” Yerel HTML modu etkin: %s", args.local)
        html = get_local_file(args.local)
        if not html:
            log.error("Yerel dosya aÃ§Ä±lamadÄ±, Ã§Ä±kÄ±lÄ±yor.")
            sys.exit(2)
        # base_url'i local dosyaya gÃ¶re varsayÄ±lan kullan
        # (kullanÄ±cÄ± manuel olarak base_url veremez bu sÃ¼rÃ¼mde)
        base_url = "https://dizi21.life"
    else:
        log.info("Manuel URL saÄŸlanmadÄ±, kategoriler otomatik taranÄ±yor...")
        aktif = aktif_domain_bul(start=args.domain_start, end=args.domain_end)
        if not aktif:
            log.error("HiÃ§bir aktif domain bulunamadÄ±! (aralÄ±k: %d..%d)", args.domain_start, args.domain_end)
            sys.exit(3)
        base_url = aktif
        # Kategori sayfasÄ±nÄ± indir
        html = None

    tum_linkler = []

    if args.local and html:
        # local Ã¼zerinden parse et
        tum_linkler = parse_links(html, base_url)
        log.info("Yerel dosyadan %d iÃ§erik bulundu.", len(tum_linkler))
    else:
        # web mod: kategorileri indir
        kategori_url = [f"{base_url}/diziler", f"{base_url}/filmler"]
        for kategori in kategori_url:
            log.debug(f"Kategori taranÄ±yor: {kategori}")
            html_k = get_page(kategori)
            if not html_k:
                log.warning("Kategori sayfasÄ± alÄ±namadÄ±: %s", kategori)
                continue
            # debug iÃ§in istersen local debug dosyasÄ± yazabilirsiniz (opsiyonel)
            # with open("debug.html", "w", encoding="utf-8") as f: f.write(html_k)
            links = parse_links(html_k, base_url)
            log.info("%s iÃ§inde %d iÃ§erik bulundu.", kategori, len(links))
            tum_linkler.extend(links)

    tum_linkler = sorted(set(tum_linkler))
    log.info("Toplam %d iÃ§erik sayfasÄ± bulundu.", len(tum_linkler))

    # Playlist oluÅŸtur
    try:
        with open(args.output, "w", encoding="utf-8") as f:
            f.write("#EXTM3U\n")
            for link in tum_linkler:
                title = link.rstrip("/").split("/")[-1] or link
                f.write(f"#EXTINF:-1,{title}\n")
                f.write(f"{link}\n")
                # kÃ¼Ã§Ã¼k gecikme, aÅŸÄ±rÄ± istekten kaÃ§Ä±nmak iÃ§in
                time.sleep(0.02)
        log.info("âœ… Playlist oluÅŸturuldu: %s", args.output)
    except Exception as e:
        log.error("Ã‡Ä±kÄ±ÅŸ dosyasÄ± yazÄ±lamadÄ±: %s (%s)", args.output, e)
        sys.exit(4)

if __name__ == "__main__":
    main()
